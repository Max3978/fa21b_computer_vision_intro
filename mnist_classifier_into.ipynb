{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = gzip.open(\"mnist.pkl.gz\")\n",
    "((x_train, y_train), (x_valid, y_valid), _ )=pickle.load(file, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f226c050460>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_image=x_train[7]\n",
    "sample_image=sample_image.reshape((28, 28))\n",
    "plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Überlegungen zur Netzarchitektur\n",
    "\n",
    "InputNeuronen: 784\n",
    "OutputNeuronen: 10\n",
    "\n",
    "    - Neuron 0 --> wenn der Wert hier am höchsten ist. Dann soll es eine 0 sein\n",
    "    - Neuron 1 --> wenn der Wert hier am höchsten ist. Dann soll es eine 1 sein\n",
    "    - Neuron 2 --> wenn der Wert hier am höchsten ist. Dann soll es eine 2 sein\n",
    "    - Neuron 3 --> wenn der Wert hier am höchsten ist. Dann soll es eine 3 sein\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(x_train, dtype=torch.long)\n",
    "x_valid = torch.tensor(x_valid, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.long)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell bauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MySimpleNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=784, hidden_size=(400, 650, 900), output_size=10):\n",
    "        \n",
    "        super(MySimpleNN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0])\n",
    "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.fc3 = nn.Linear(hidden_size[1], hidden_size[2])\n",
    "        self.fc4 = nn.Linear(hidden_size[2], output_size)\n",
    "\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_size[0])\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_size[1])\n",
    "        self.bn3 = torch.nn.BatchNorm1d(hidden_size[2])\n",
    "\n",
    "    def forward(self, xb):\n",
    "        z2 = self.fc1(xb)\n",
    "        a2 = torch.relu(z2)\n",
    "        a2_bn = self.bn1(a2)\n",
    "\n",
    "        z3 = self.fc2(a2_bn)\n",
    "        a3 = torch.relu(z3)\n",
    "        a3_bn = self.bn2(a3)\n",
    "\n",
    "        z4 = self.fc3(a3_bn)\n",
    "        a4 = torch.relu(z4)\n",
    "        a4_bn = self.bn3(a4)\n",
    "\n",
    "        y_hat = torch.relu(self.fc4(a4_bn))\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size):\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        yield (x[i:i+batch_size], y[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m MySimpleNN()\n\u001b[1;32m      2\u001b[0m xb, yb \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(get_batch(x_train, y_train, \u001b[39m64\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(model(xb))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m, in \u001b[0;36mMySimpleNN.forward\u001b[0;34m(self, xb)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, xb):\n\u001b[0;32m---> 17\u001b[0m     z2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(xb)\n\u001b[1;32m     18\u001b[0m     a2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(z2)\n\u001b[1;32m     19\u001b[0m     a2_bn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(a2)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "model = MySimpleNN()\n",
    "xb, yb = next(get_batch(x_train, y_train, 64))\n",
    "print(model(xb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m loss_sum \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m xb, yb \u001b[39min\u001b[39;00m get_batch(x_train, y_train, batch_size):\n\u001b[0;32m---> 23\u001b[0m     loss_sum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train(xb, yb)\n\u001b[1;32m     24\u001b[0m loss_epoch \u001b[39m=\u001b[39m loss_sum \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(x_train)\n\u001b[1;32m     25\u001b[0m losses\u001b[39m.\u001b[39mappend(loss_sum\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(x_train))\n",
      "Cell \u001b[0;32mIn[137], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(xb, yb)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(xb, yb):\n\u001b[0;32m---> 11\u001b[0m     y_hat \u001b[39m=\u001b[39m model(xb)\n\u001b[1;32m     12\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(y_hat, yb)\n\u001b[1;32m     13\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[135], line 17\u001b[0m, in \u001b[0;36mMySimpleNN.forward\u001b[0;34m(self, xb)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, xb):\n\u001b[0;32m---> 17\u001b[0m     z2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(xb)\n\u001b[1;32m     18\u001b[0m     a2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(z2)\n\u001b[1;32m     19\u001b[0m     a2_bn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(a2)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "model = MySimpleNN()\n",
    "lr = 0.0001\n",
    "batch_size = 256\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "def train(xb, yb):\n",
    "    y_hat = model(xb)\n",
    "    loss = loss_fn(y_hat, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "losses=[]\n",
    "\n",
    "for epoch in (range(400)):\n",
    "    loss_sum = 0\n",
    "\n",
    "    for xb, yb in get_batch(x_train, y_train, batch_size):\n",
    "        loss_sum += train(xb, yb)\n",
    "    loss_epoch = loss_sum / len(x_train)\n",
    "    losses.append(loss_sum/len(x_train))\n",
    "    scheduler.step(loss_epoch)\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch: \", epoch, \"Loss: \", loss_epoch, \"LR: \", optimizer.param_groups[0]['lr'])\n",
    "\n",
    "plt.plot(losses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
